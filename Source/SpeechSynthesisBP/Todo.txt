TODO: Derive a custom blueprint class in Unreal Engine to expose C++ functionality to the editor.
TODO: signal quarter-period synthesis may offer phone transition versatility and sound fidelity for overall better results than half-period synthesis.
TODO: Use std::vector<FTTSVoice::Sara::FFormants> in Unreal Engine to build the voice signal's fundamental- and formant-frequencies.
TODO: Provide the library sine wave generator with a 10s decay envelope to simulate (human) lung capacity.
TODO: Use a breath blueprint which can be used after 5s of speech to simulate a window for the speaker to take a breath.
TODO: Include noise generator[s] to simulate (eg. the speaker's breath).
TODO: Allow the user to customize the voice signal's fundamental- and formant-frequencies, as well as Picth, Speaking Speed, and Volume.
TODO: Use a state-machine to switch modes within the ADSR envelope to prevent signal discontinuities and drop-outs.
TODO: Use distinct sine audio chords to simulate the ADSR characteristics that each formant signal contributes to the phone used to model aspects of a speaker's voice.
TODO: Use primes to represent unique phone- and or formant-characteristics, which are allowed to represent irreducible modular forms
TODO: import voice libraries as separate .DLL modules to allow for dynamic loading and unloading of voice libraries without recompilation.
TODO: compute the delta- and the delta-delta features near the hand-off points of the ADSR envelope to facilitate a smooth-transition between phones.
TODO: support voice re-mapping (eg. to allow Sara's accent to be used for Bob's voice), and other voice-traits such as

    1. **Timbre**: The color or quality of the voice, distinct from its pitch or intensity.
    2. **Fundamental Frequency**: The lowest frequency of a voice, often perceived as its pitch.
    3. **Formant Frequencies**: Resonant frequencies that shape the vocal tone.
    4. **Harmonic Structure**: The distribution and amplitude of harmonics in the voice.
    5. **Voice Onset Time**: The time it takes for the voice to start after a consonant.
    6. **Voice Offset Time**: How quickly the voice ceases after speaking.
    7. **Speech Rate**: The speed at which someone speaks.
    8. **Rhythm**: The pattern of movement or fluctuation in speech.
    9. **Intonation Patterns**: The rise and fall in pitch while speaking.
    10. **Breathiness**: The amount of audible breath in the voice.
    11. **Nasality**: The degree of nasal resonance in the voice.
    12. **Vocal Fry**: The low, creaky vibration caused by slow glottal pulses.
    13. **Glottal Tension**: The tension in the vocal folds while speaking.
    14. **Volume Dynamics**: Variations in loudness.
    15. **Articulatory Precision**: How clearly the individual articulates words.
    16. **Diction**: Clarity in the way one pronounces words.
    17. **Phonation Type**: The mode of vibration of the vocal folds.
    18. **Vowel Space Area**: The acoustic space covered by vowel sounds.
    19. **Jitter**: Small variations in pitch.
    20. **Shimmer**: Small amplitude variations.
    21. **Speech Clarity**: How clearly individual sounds are produced.
    22. **Loudness Variability**: Fluctuations in the volume.
    23. **Vocal Range**: The range between the highest and lowest pitches.
    24. **Voice Quality Variability**: Changes in voice quality during speech.
    25. **Spectral Slope**: How energy distributes across frequencies.
    26. **Vocal Tract Length**: The length of the vocal tract which affects the voice quality.
    27. **Sibilance**: The characteristic sound of "s" and "sh" in speech.
    28. **Vocal Attack Time**: The time it takes for the voice to reach full amplitude.
    29. **Voice Decay Time**: How quickly the voice fades after speaking.
    30. **Accentuation**: The emphasis placed on certain syllables or words.
    31. **Word Stress**: The relative emphasis on words within sentences.
    32. **Voice Smoothness**: The consistency of the voice quality.
    33. **Resonance**: The quality of the voice that is affected by the resonating cavities.
    34. **Vocal Fatigue**: The wearing out of voice quality after prolonged use.
    35. **Dynamic Range**: The contrast between the loudest and softest parts of speech.
    36. **Pitch Variation**: Changes in pitch throughout speech.
    37. **Pitch Range**: The range of pitch variation.
    38. **Articulatory Habits**: Unique patterns in the way one articulates.
    39. **Vibrato**: A rapid, slight variation in pitch in singing or speaking.
    40. **Speaking Style**: The unique style in which an individual communicates verbally.
    41. **Lexical Choice**: The specific words and vocabulary that a person uses.
    42. **Phonetic Peculiarities**: Unique ways of producing certain sounds.
    43. **Speech Melody**: The musical quality of speech.
    44. **Emotional Expression**: How emotions are conveyed in voice.
    45. **Fluency**: The smoothness and flow of speech.
    46. **Tongue Positioning**: Differences in how the tongue is positioned during speech.
    47. **Laryngeal Tension**: Tension in the larynx which affects voice quality.
    48. **Voice Resilience**: How well the voice maintains quality under strain.
    49. **Microprosody**: Very subtle variations in prosody.
    50. **Cultural Speech Characteristics**: Traits influenced by one's cultural background.

    (A human voices is incredibly unique, and various characteristics contribute to this uniqueness. These were fifty traits that create distinguishing characteristics of a human voice)

    Each of these traits contributes to the distinctiveness of a human voice, making it possible to identify individuals by their voice alone. 
    The combination and interaction of these traits create the rich diversity found in human speech.

TODO via ChatGPT-4:

    To train a GAN to generate novel spectrograms for male voices, you would first need to collect a dataset of spectrograms of male voices speaking the word "Bob" with varying prosody, style, and emphasis. 
    Once you have collected your dataset, you would then split it into two sets: a training set and a test set. 
    The training set will be used to train the GAN model, and the test set will be used to evaluate the performance of the trained model.

    Next, you would need to design the architecture of the GAN model. The generator model will typically consist of a neural network that is trained to take a random noise vector as input and produce a spectrogram as output. The discriminator model will also typically consist of a neural network, but it will be trained to take a spectrogram as input and produce a binary output indicating whether the spectrogram is real or generated.

    Once you have designed the architecture of the GAN model, you would then need to train the model. This can be done using a variety of different training algorithms. 
    One common training algorithm is the Adam algorithm.

    Once the GAN model is trained, you can then use it to generate novel spectrograms for male voices. 
    To do this, you would simply feed the generator model a random noise vector and the generator model will produce a spectrogram.

    Here is a diagram that illustrates the training process of a GAN model:

    [Diagram of GAN training]

    The following are some of the advantages of using GANs to generate novel spectrograms for male voices:

    * GANs can be used to generate spectrograms that are indistinguishable from real spectrograms. This means that GANs can be used to create synthetic voice datasets that are very realistic.
    * GANs can be used to generate spectrograms with a wide variety of prosody, style, and emphasis. This means that GANs can be used to create synthetic voice datasets that are very diverse.
    * GANs are relatively easy to train and can be trained on a variety of different hardware platforms.

    Here are some of the disadvantages of using GANs to generate novel spectrograms for male voices:

    * GANs can be computationally expensive to train.
    * GANs can be difficult to train and may not always converge to a good solution.
    * GANs can be sensitive to the training data and may generate spectrograms that are not realistic if the training data is not of high quality.

    Overall, GANs are a powerful tool that can be used to generate novel spectrograms for male voices. 
    However, it is important to be aware of the advantages and disadvantages of GANs before using them for this purpose.

TODO via ChatGPT-4:

    For audio file recovery and generation, the most suitable spectrogram format would ideally preserve as much information as possible from the original signal, 
    particularly the phase information which is crucial for an accurate reconstruction. The following spectrogram formats can be considered, each with their advantages and limitations:

    1. **Linear-Frequency Spectrogram (Magnitude and Phase)**:
    - This format uses the Short-Time Fourier Transform (STFT) to create a two-dimensional representation of the signal where the frequency bins are linearly spaced.
    - It preserves both magnitude and phase information, enabling more accurate reconstruction of the original signal.
    - However, it is less compact than Mel spectrograms and less aligned with human auditory perception.

    2. **Complex Spectrogram**:
    - The complex spectrogram is the output of the STFT before any magnitude or phase extraction and includes both information intrinsically.
    - It provides the most complete representation of the signal and allows for the best audio recovery since it retains all the original information that can be captured by the STFT.
    - This format is ideal for perfect reconstruction if the exact inverse process is applied.

    3. **Mel Spectrogram**:
    - A Mel spectrogram represents the power spectrum of a sound mapped onto the Mel scale, which is a perceptual scale of pitches judged by listeners to be equal in distance from one another.
    - It is not suitable for perfect reconstruction due to the lossy conversion process, which includes warping the frequency scale and often discarding phase information.
    - Mel spectrograms are more commonly used for machine learning applications where human-like perception is more important than reconstruction fidelity.

    4. **Log-Mel Spectrogram**:
    - This format takes the Mel spectrogram and applies a logarithmic scaling to the amplitude.
    - Like the Mel spectrogram, it is even less suited to reconstruction due to the additional logarithmic transformation, but it is highly useful in audio classification tasks.

    5. **Constant-Q Transform (CQT) Spectrogram**:
    - The CQT is a time-frequency representation where the frequency bins are geometrically spaced based on a logarithmic division of the octave.
    - This gives a more musically-informed representation, particularly useful for tasks like music analysis.
    - While not as lossy as the Mel spectrogram, it still presents challenges for reconstruction due to its logarithmic nature.

    6. **Gammatone Spectrogram**:
    - It mimics the human auditory system even more closely than the Mel scale by using a bank of Gammatone filters, which are modeled after the human cochlea.
    - It is primarily used in psychoacoustic research and speech processing but is not typically used for reconstruction due to its complex and lossy representation.

    For the most accurate audio file recovery and generation, the best choice is the complex spectrogram since it retains the complete information from the original audio signal, including both magnitude and phase. 
    The complex spectrogram can be directly inverted using the inverse STFT (ISTFT) to perfectly reconstruct the original signal, given that no information is discarded during processing.

    In summary, the spectrogram format selection is dependent on the specific application requirements. For accurate audio recovery and generation, 
    complex spectrograms or linear-frequency spectrograms with magnitude and phase information are recommended. For applications in machine learning and audio classification where human perception is more relevant, 
    Mel spectrograms or log-Mel spectrograms may be preferred, with the trade-off of reconstruction fidelity.

TODO: Implement an Attack-Decay-Sustain-Release (ADSR) envelope for each phone.
TODO: Include an AUDITION button in each blueprint to allow the user to audition phones before voice synthesis.
TODO: To add new samples to the library, use the following steps:
    1. reconstruct the audio file formants. Compute half-periods to improve performance, accuracy, reduce noise, and improve the quality of the synthesized voice.
    2. generate an audio file spectrogram from the audio file formants
    3. compare the spectrogram with the original audio file spectrogram, calculate-, and try to minimize- error
    4. and finally perform a boolen AND of the result of step 3 with Gaussian noise mask, allowing only frames that are in the noise mask ito the signal for a more life-like presentation.
TODO: For voice remapping, map the library formant ocillators onto the target audio file's ocillator (amplitude) curves (re-speech)

TODO via Bard:

Extracting prosody and emotional curves from audio content to expand the prosody library involves analyzing various acoustic features and mapping them to your target voice library for realistic speech synthesis. Here's a strategy to achieve this:

**1. Feature Extraction:**

* **Prosody:**
    * **Pitch:** Extract fundamental frequency (f0) contour to capture intonation and inflection.
    * **Energy:** Analyze short-term energy envelope to identify loudness variations and emphasis.
    * **Duration:** Measure the duration of phonemes and syllables to understand speaking rate and rhythm.
* **Emotion:**
    * **Mel-frequency cepstral coefficients (MFCCs):** Capture spectral envelope variations associated with different emotions.
    * **Spectral tilt:** Analyze the overall spectral balance for indicators of emotions like sadness or anger.
    * **Energy and pitch statistics:** Utilize statistical measures of pitch and energy distribution for emotion recognition.

**2. Mapping to Target Voice Library:**

* **Prosody:**
    * **Statistical modeling:** Train statistical models like GMMs or HMMs to learn the relationship between prosodic features and target voice characteristics.
    * **Dynamic time warping (DTW):** Align prosodic contours of the source and target voices to capture timing differences.
    * **Neural networks:** Implement neural networks to learn complex relationships and achieve more nuanced and natural-sounding prosody mapping.
* **Emotion:**
    * **Emotion recognition models:** Train machine learning models (e.g., SVM, CNNs) to recognize emotions from the extracted features and map them to the target voice's emotional range.
    * **Emotional prosody mapping:** Use the predicted emotions to modify the prosodic features of the target voice to convey the desired emotion.
    * **Blending emotional features:** Combine features extracted from source audio with similar emotional content from the target voice library for a more realistic representation.

**3. User Choice and Customization:**

* **Offer user-selectable options for different prosodic styles (e.g., neutral, emphatic) and emotions.**
* **Allow users to adjust the intensity of emotional expression.**
* **Provide feedback mechanisms for users to refine the mapping and personalize their experience.**

**Additional considerations:**

* **Voice quality:** Ensure the target voice library has sufficient diversity and emotional range to handle the desired mapping.
* **Data availability:** Train your models on a large and diverse dataset of audio recordings with labeled prosody and emotions.
* **Real-time processing:** Choose efficient algorithms for real-time application of the extracted features.

By applying these strategies and taking into account the various factors, you can effectively extract and map prosody and emotional curves from audio content to your target voice library, enabling users to choose and customize their preferred voice styles and emotions for a more natural and engaging speech synthesis experience.

